{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble:\n",
    "    Ensemble basically means combining multiple models and train them using our dataset and predict the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Techniques:\n",
    " 1. Bagging (Bootstrap Aggregation) -> Random Forest\n",
    " 2. Boosting -> AdaBoost, Gradient Boosting and XGBoost (Xtreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging:\n",
    "In Bagging, we basically create multiple models (M1,M2,M3,...,Mk) and provide each of teh model with a sample of our dataset\n",
    "D'.           Say, M1->D'1,M2->D'2,...Mk->D'k. Let the no. of rows in the dataset be 'n', then the size of the sample records/rows provided for the Mk will be 'm' where m<n. Note that the data is reshuffled before resampling. This is called Row Sampling with Replacement (Replacement-> shuffling). The models are trained with respect to the given data. Now every model predicts the value using the test data.If the problem is regarding classification, then we use Voting (i.e., the class with the majority becomes the prediction) and if the problem is related to regression, then our prediction turns out to be the avg. value of all the individual predictions.\n",
    "Row sampling and Replacement is called Bootstrap and the using of Voting Classifier or avg./median of the predictions is called the Aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree (Key Points):\n",
    "Decision tree works on the principle of Bagging where both Row Sampling and Feature Sampling occurs.\n",
    "<br>Decision Tree has:\n",
    "<ul>\n",
    "<li>Low Bias:<p style='color:green'>Low bias basically says that if you are training your decision tree to its complete depth then your decision tree gets well trained for its training dataset i.e., your error for the training dataset will be less.</p></li>\n",
    "<li>High Vaiance:<p style='color:green'>High Variance basically says that if you test your decision tree with the test data, you will have large errors</p></li>\n",
    "</ul>\n",
    "<p style='color:red'>In short, if your decision tree gets trained to its complete depth, then your model gets overfit.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='font-style:italic'>So What happens in Random Forest?</h2>\n",
    "<p>Now that we know decision trees contain high variance, when we combine the results of various decision trees, the high variance gets converted into low variance.</p>\n",
    "<p>If your dataset contains 1000 rows and you train the model using random forest (multiple decision trees). Now you change 200 rows in the original dataset. This will not have a large impact in the accuracy of the model because the multiple decision trees use multiple samples of datasets and hence those 200 records will be distributed among the decision trees and its impact will subside. Hence, even your test data will give similar accuracy.<br>Hence, Random Forest is one of the best ML Algorithms.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence,<br>\n",
    "We convert a Decision Tree (low bias and High Variance) to Random Forest (low bias and low varaince) to increase its accuracy on both training and test data. Make sure to use both row sampling and feature sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color:red;font-style:italic'>Hyper Parameter we tune in Random Forest is the number of Decision Trees to be used during Training the data.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

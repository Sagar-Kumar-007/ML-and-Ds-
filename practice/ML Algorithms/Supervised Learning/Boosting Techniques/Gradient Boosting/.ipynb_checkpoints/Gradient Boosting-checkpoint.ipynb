{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Gradient Boosting: (Not original Maths)</h1><br><p>Consider you have a dataset with features as Experience and Degree with output feature as salary. To perform Gradient Boosting, you need to follow the steps:\n",
    "<ol>\n",
    "<li><p>Compute Base Model which will give you one output. The output will be the avg. of all the salaries. Then you make a new column named y^ (y-hat) which will contain the avg salary.<br><img src='step1.png' width=300><br></p></li>\n",
    "<li><p>Compute the errors / pseudo residual. This calculation involves loss function but for now we are taking actual value subtracted with predicted value.<br><img src='step2.png' width=300><br></p></li>\n",
    "<li><p>Now you create a new decision tree with inputs as independent features (experience, salary) and outputs as the residuals (R1). A point to note that now you test your data with the same independent features, you will get output as residuals (say R2).<br><img src='step3.png' width=300><br></p></li>\n",
    "<li><p>So now that we got our residual values, we need to find our predicted salary. To find our predicted salary, we can add the residual with the base model prediction (i.e., avg of all salaries). Lets take the first row, avg. salary is 75k and the residual is -23k. Hence predicted salary will be (75-23)=52k which is very near to the expected value (i.e., 50k). Since we predicted our value with high accuracy using training data using only one decision tree, our model will tend to be overfit. So to rectify this we multiply the residual with a parameter called 'Learning Rate' (ranging between 0 and 1). This will decrease the variance since now our predicted salary would be (say learning rate=0.1) (75+(0.1)*(-23))=73.7</p></li>\n",
    "<li><p>Now since our predicted salary is not near to our original salary, we create another decision tree using the independent features (experience and degree) and dependent feature (R2). Then we calculate R3 and check whether our predicted salary is near to our original value or not. If not we repeat the process again.</p></li>\n",
    "<li><p>We can formulate a general formula to calculate the predicted value (where alpha=learning rate and hi(x)= output/residual)<br><img src='genformula.png' width=300><br></p></li>\n",
    "<li><p>The Aplha value is to be estimated using HyperParameter Tuning. Note that we are adding all the residuals and not just using the final residual.</p></li>\n",
    "</ol></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We call these trees as sequential trees because we first create a base model, and then we create the trees sequentially i.e., \n",
    "# we create the first tree then we create the second tree and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>PseudoAlgorithm</h3><br><img src='pseudoalgorithm.png' width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our first step was to intialize our base model with a constant value (avg value). But actually, we initialize it using\n",
    "# a formula as mentioned in the step 1 of our pseudo algorithm. In this formula, L() is the loss function with parameters yi\n",
    "# (original salaries) and gamma (predicted value/ y-hat). Since our problem is of regression, we use regression loss function\n",
    "# which is summed up using all values of actaul salaries, i.e.,summation of [ (1/2)*(y-y.hat)^2 ] for all y values \n",
    "# (i.e., actual salaries). Since the formula contains arg min of gamma we should find the minimum value of the sum\n",
    "# (differentating w.r.t y.hat) and then equating it to zero in order to get the value of the y.hat. Now we initialize our\n",
    "# base model with y.hat as the constant value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the second step of the pseudo algorithm, m=no. of trees. Now, for 1 to m itearions we basically compute the pseudo error \n",
    "# using the formula in the pseudo algorithm (2.1). This formula is basically the derivative of the loss function with a minus \n",
    "# sign. Since our step after finding the constant value for the base model was to find the residual via subtracting y and y.hat \n",
    "# (constant value) we derivate the loss function [ (1/2)*(y-y.hat)^2 ] and get the ans as [y-y.hat]. So, we have derived\n",
    "# the residual R1. In the formula, F(xi) is the previous function i.e., y.hat. In rim, i is the row no. and m is the tree no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we train our decision trees with independent features (experience, salary) and dependent feature (residual).\n",
    "# In the third step of the second step of the pseudo algorithm, Fm-1(xi) is the previous model output. The value of this is used\n",
    "# as learning rate. The fourth step of the second step is similar to the one as said said before.\n",
    "# Note that if you want to set the learning rate using the hyperparameter tuning, you can skip the third step of step 2\n",
    "# and directly use the fourth step of step 2 where the learning rate will be assigned by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingRegressor\n",
    "# clf=GradientBoostingRegressor(loss= loss function to be used, learning_rate= To be initialized using hyperparameter tuning,\n",
    "# n_estimaters= no. of trees to be used (generally large no. of trees results in less chance of overfitting the tree))\n",
    "# clf.fit(X,y)\n",
    "# clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly with the GradientBoostClassifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

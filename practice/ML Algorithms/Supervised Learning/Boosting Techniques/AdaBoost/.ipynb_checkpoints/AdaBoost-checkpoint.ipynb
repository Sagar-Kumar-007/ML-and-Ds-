{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Boosting:</h1><img src='working.png' width=500><p>Consider 'D' to be the dataset and BL1, BL2,... BLk to be Base Learners/models to be trained (decision trees). The working of boosting techniques is that first a model takes few records from the dataset and trains itself. Then that model tests its accuracy for the entire dataset. Now, it consider the rows which were wrongly predicted and then the second model only takes those rows which were wrongly predicted and predicts the entire dataset.  The rows wrongly predicted are trained in the third model and the process is repeated for 'n' repeations. Note that the value of 'n' is to be entered as a parameter.</p1>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>AdaBoost:</h2><p>AdaBoost is little bit different from the original boosting technique.<br>AdaBoost Working:<br>Consider that you have a dataset with three features (f1,f2,f3) and seven rows. You also have a output column. Steps followed during Adaboost:<ol><li>All records will get some 'Sample Weight' whose formula is (1/n) where n=no. of rows. You assign each row with same sample weight (i.e., 1/7 in this case)</li><li><p>We create our first base learner. We use decision trees to create our base learners in AdaBoost. Unlike Random Forests, we create a decision tree with depth='1'. These decision trees are called Stumps. Now, we make these stumps for every feature in the decision tree (for this dataset, we make 3 stumps with root node as f1, f2, f3). Then for all the stumps, we check their entropy and gini impurity and select the stump with the least entropy and make it as the first base learning model.</p></li><li><p>Now that you have selected your first base learning model, we check the number of correctly and incorrectly classified rows. Suppose that our model correctly classified 4 rows and incorrectly classified one row. Now we have to calculate the total error of the incorrectly classified record. The method to check the total error of the incorrectly classified record is that by summing up all the sample weights of the incorrectly classified rows (i.e., since we have only one wrongly classified row, our error would be 1/7. If the wrongly classified rows were 'n' which is less than or equal to total no. of rows, then our error would be n/7 (since we have seven rows.))</p></li><li><p>In this step, we try to find the performance of the stump. The formula is <br><img src='formula.png'><br>Note that T.E in the formula indicates the total error. Performance basically indicates how well the tree has classified the data. For our dataset with one error, the performance will be 0.896 (substitute 1/7 in the above formula). We calculate the T.E and performance to update the weights for each row (initially it was 1/7 for each row for our dataset).</p></li><li><p>Since, in boosting we train our second base learner with the wrongly classified records of our first base learner, in AdaBoost we increase the weights of the wrongly classified rows and decrease the weights of the correctly classified rows. Now we have to update the weights. We use the formula (for incorrectly classified rows): new_weights=old_weight*e^(performance) and new_weights=old_weight*e^(-performance) (for correctly classified rows). Hence, the new weight for the incorrectly classified row will be 0.349 and 0.05 for correctly classified rows. Now we update all the values to the respected rows.</p></li><li><p>Now we must observe that the sum of the sample weights was '1' since the formula was (1/n) but the summation of the updated weights is not '1'. So we have to divide a number ( which is the summation of all the updated weights ) with the updated weights such that the summation becomes '1'. The sum in this case is '0.68'. The weights are now called as Normalized Weights.<br><img src='steps.png' width=500><br></p></li><li><p>Considering the normalized weights we will create a new dataset. Now that dataset based on the normalized weights will train using the wrongly classified values. To create that dataset, first we label the columns names similar to the first dataset and then based on the normalized weights, we will try to divide the first dataset into buckets (i.e., we put the weights onto a number line type fo format/bins)<br><img src='buckets.png' width=300><br>Now our algorithm will run 8 iterations to select different records from the older dataset. We take random values between 0 and 1 and check the bucket in which this value lies.  Since, the size of the bucket for the wrongly classified data will be large the probability of the number being in the wrong bucket will also be large. Then, the row of that bucket is entered in the new dataset.</p></li><li><p>Now our new dataset is created. We will repeat the above procedure for this dataset and you will observe that our error will decrease and the total process is repeated for 'n' iterations (whose value is to be given as a parameter in the sklearn AdaBoost). Finally, our model will have very little error. </p></li><li><p>During testing our data, we will give the test data to each stump and then predict its value. If it is a classification problem, we will select the mojority element based on voting and if it is a regression problem, then we will take the average or medain of all the predicted values and consider it to be the final predicted value.</p></li></ol></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='font-style:italic;color:blue'>Boosting technique is like a group of friends preparing for a exam, where each person studies individual chapters well and hence the entire group performs better in the exam.</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf=AdaBoostClassifier(n_estimaters=50)  # n_estimaters is the no. of times the process of Adaboosting should repeat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf.fit(X_train,y_train)\n",
    "# clf.score(X_test,y_test)\n",
    "# y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AdaBoostRegressor is also simmilar to AdaBoostClassifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
